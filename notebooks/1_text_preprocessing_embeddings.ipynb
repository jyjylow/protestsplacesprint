{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiayilow/Desktop/CASA/Dissertation/mapping_reform/diss-venv311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import pyarrow.parquet as pa\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# text stuff\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import StaticEmbedding\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# mathy stuff\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jiayilow/Desktop/CASA/Dissertation/mapping_reform'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory if necessary\n",
    "os.getcwd()\n",
    "# os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading in NCSE v2.0 data\n",
    "this data is downloaded from https://rdr.ucl.ac.uk/articles/dataset/NCSE_v2_0_A_Dataset_of_OCR-Processed_19th_Century_English_Newspapers/28381610."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: change file path if necessary\n",
    "\n",
    "parquet_files = {\n",
    "    \"The English Woman's Journal\": \"data/ncse_raw/English_Womans_Journal_issue_PDF_files.parquet\",\n",
    "    \"The Tomahawk\": \"data/ncse_raw/Tomahawk_issue_PDF_files.parquet\",\n",
    "    \"The Leader\": \"data/ncse_raw/Leader_issue_PDF_files.parquet\",\n",
    "    \"The Monthly Repository\": \"data/ncse_raw/Monthly_Repository_issue_PDF_files.parquet\",\n",
    "    \"The Northern Star\": \"data/ncse_raw/Northern_Star_issue_PDF_files.parquet\",\n",
    "    \"The Publisher's Circular\": \"data/ncse_raw/Publishers_Circular_issue_PDF_files.parquet\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup and functions for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_words = 5     # setting min words\n",
    "token_limit = 384 # mpnet truncates text past 384 tokens by default\n",
    "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# %%\n",
    "# whole buncha functions for preprocessing\n",
    "\n",
    "def fix_encoding_errors(text):\n",
    "    \"\"\"Fix common UTF-8 misencoding issues.\"\"\"\n",
    "    replacements = {\n",
    "        '‚Äî': '—',\n",
    "        '‚Äú': '“',\n",
    "        '‚Äù': '”',\n",
    "        'â€œ': '“',\n",
    "        'â€\\x9d': '”',\n",
    "        'â€˜': '‘',\n",
    "        'â€™': '’',\n",
    "        'â€“': '–',\n",
    "        '\\n': ' ',\n",
    "    }\n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "    \n",
    "def split_by_token_limit(text, token_limit=384):\n",
    "    \"\"\"Split long text into chunks within the token limit using tokenizer.\"\"\"\n",
    "    \n",
    "    # input validation\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "\n",
    "    # tokenising\n",
    "    tokens = tokenizer(text, padding=False, truncation=False, return_offsets_mapping=True)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    offsets = tokens[\"offset_mapping\"]\n",
    "\n",
    "    # Handle edge cases\n",
    "    if len(input_ids) <= token_limit:\n",
    "        return [text.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(input_ids):\n",
    "        end = min(start + token_limit, len(input_ids)) # the smaller of the 2\n",
    "        \n",
    "        # Get the start/end char positions of this token span\n",
    "        start_char = offsets[start][0] if start < len(offsets) else len(text) # ensuring starting token index exists within bounds, otherwise fallback to end of text\n",
    "        end_char = offsets[end - 1][1] if end <= len(offsets) else len(text) # ensuring ending token index exists within bounds, otherwise fallback to end of text\n",
    "\n",
    "         # Ensure we don't go beyond text boundaries\n",
    "        start_char = max(0, min(start_char, len(text)))\n",
    "        end_char = max(start_char, min(end_char, len(text)))\n",
    "\n",
    "        \n",
    "        chunk = text[start_char:end_char].strip()\n",
    "\n",
    "        # keeping only non-empty chunks\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def clean_df(df_raw, \n",
    "             publication_name='unknown_publication', \n",
    "             output_path='preprocessed_articles.parquet',\n",
    "            min_words=5,\n",
    "            token_limit=384):\n",
    "    \n",
    "    # 1. Filter only ‘text’ rows\n",
    "    df = df_raw[df_raw['class'] == 'text'][['issue_id', 'content']].copy()\n",
    "\n",
    "    # 2. Clean UTF-8/regex artifacts\n",
    "    df['content'] = df['content'].apply(fix_encoding_errors)\n",
    "\n",
    "    # 3. Keep only articles with more than min_words\n",
    "    content_lens = df['content'].str.count(r'\\w+')\n",
    "    short_articles = df[content_lens < min_words]\n",
    "    df = df[content_lens >= min_words]\n",
    "    print(f\"{len(short_articles)} articles dropped - fewer than {min_words} words.\")\n",
    "\n",
    "    # 4. Drop duplicate content\n",
    "    pre_duplicatedrop = len(df)\n",
    "    df = df.drop_duplicates(subset='content')\n",
    "    post_duplicatedrop = len(df)\n",
    "    print(f\"{pre_duplicatedrop - post_duplicatedrop} duplicate articles removed.\")\n",
    "\n",
    "    # 5. Split by token count\n",
    "    split_articles = []\n",
    "    problematic_articles = []\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            chunks = split_by_token_limit(row['content'], token_limit)\n",
    "            for chunk in chunks:\n",
    "                split_articles.append({\n",
    "                    'issue_id': row['issue_id'],\n",
    "                    'content': chunk\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {idx}: {e}\")\n",
    "            problematic_articles.append(idx)\n",
    "            continue\n",
    "                \n",
    "\n",
    "    df_split = pd.DataFrame(split_articles)\n",
    "\n",
    "    # 6. Create column from date published\n",
    "    # i found that there's a misformatted date, '1852-31-07', in The Leader's file. fixing this below:\n",
    "    df_split['issue_id'] = df_split['issue_id'].str.replace('1852-31-07', '1852-07-31', regex=False)\n",
    "\n",
    "    df_split['pub_date'] = pd.to_datetime(\n",
    "        df_split['issue_id'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')[0],\n",
    "        format='%Y-%m-%d', errors='coerce'\n",
    "    )\n",
    "\n",
    "    # 7. Add publication column\n",
    "    df_split['publication'] = publication_name\n",
    "\n",
    "    # Save as Parquet\n",
    "    df_split.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {len(df_split)} preprocessed articles to {output_path}.\")\n",
    "\n",
    "    return df_split.reset_index(drop=True), short_articles.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing publications and saving them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory to store preprocessed articles already exists.\n",
      "Preprocessed data file already exists for The English Woman's Journal. Loading it...\n",
      "Preprocessed data file already exists for The Tomahawk. Loading it...\n",
      "Preprocessed data file already exists for The Leader. Loading it...\n",
      "Preprocessed data file already exists for The Monthly Repository. Loading it...\n",
      "Preprocessed data file already exists for The Northern Star. Loading it...\n",
      "Preprocessed data file already exists for The Publisher's Circular. Loading it...\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"output_data/preprocessed_data/\"\n",
    "if not os.path.exists(output_folder):\n",
    "    print(f\"Creating directory to store preprocessed articles: {output_folder}\")\n",
    "    os.makedirs(output_folder)\n",
    "else:\n",
    "    print(\"Directory to store preprocessed articles already exists.\")\n",
    "    \n",
    "    \n",
    "# dict to store preprocessed publications\n",
    "cleaned_dfs = {}\n",
    "short_articles_dfs = {}\n",
    "\n",
    "for pub_name, path in parquet_files.items():\n",
    "    flat_name = pub_name.lower().replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "    output_path = f\"{output_folder}{flat_name}_preprocessed.parquet\"\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Preprocessed data file already exists for {pub_name}. Loading it...\")\n",
    "        cleaned_df = pa.read_table(output_path).to_pandas()\n",
    "        cleaned_dfs[pub_name] = cleaned_df\n",
    "\n",
    "    else: \n",
    "        print(f\"Loading and processing: {pub_name}\")\n",
    "\n",
    "        # reading and process original data\n",
    "        raw_df = pa.read_table(path).to_pandas()\n",
    "        cleaned_df, short_df = clean_df(raw_df, publication_name = pub_name, output_path = output_path)\n",
    "\n",
    "        cleaned_dfs[pub_name] = cleaned_df \n",
    "        short_articles_dfs[pub_name] = short_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assigning outputs to individual publication variables\n",
    "ewj = cleaned_dfs[\"The English Woman's Journal\"]\n",
    "thawk = cleaned_dfs[\"The Tomahawk\"]\n",
    "leader = cleaned_dfs[\"The Leader\"]\n",
    "monrepo = cleaned_dfs[\"The Monthly Repository\"]\n",
    "star = cleaned_dfs[\"The Northern Star\"]\n",
    "circ = cleaned_dfs[\"The Publisher's Circular\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating and saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory for embeddings already exists: output_data/article_embeddings/embeddings_mpnet\n",
      "Pre-computed embeddings already exist for The English Woman's Journal.\n",
      "Pre-computed embeddings already exist for The Tomahawk.\n",
      "Pre-computed embeddings already exist for The Leader.\n",
      "Pre-computed embeddings already exist for The Monthly Repository.\n",
      "Pre-computed embeddings already exist for The Northern Star.\n",
      "Pre-computed embeddings already exist for The Publisher's Circular.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# directory to store embeddings \n",
    "embedding_dir = \"output_data/article_embeddings/embeddings_mpnet\"\n",
    "\n",
    "if not os.path.exists(embedding_dir):\n",
    "    print(f\"Creating directory to store embeddings: {embedding_dir}\")\n",
    "    os.makedirs(embedding_dir)\n",
    "else:\n",
    "    print(f\"Directory for embeddings already exists: {embedding_dir}\")\n",
    "\n",
    "\n",
    "for pub, pub_df in cleaned_dfs.items():\n",
    "    filename = pub.lower().replace(\" \", \"_\").replace(\"'\", \"\").replace(\".\",\"\") + \".pkl\"\n",
    "    filepath = os.path.join(embedding_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Generating vector embeddings for {pub}...\")\n",
    "        pub_text = pub_df[\"content\"].tolist()\n",
    "        pub_date = pub_df[\"pub_date\"].tolist()\n",
    "        pub_vec = SentenceTransformer(model).encode(pub_text, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        print(f\"Vector array of shape {pub_vec.shape} obtained.\")\n",
    "        print(f\"Saving embeddings for {pub} to {filepath}...\")\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"articles\": pub_text, \n",
    "                \"dates\": pub_date,\n",
    "                \"embeddings\": pub_vec},\n",
    "                f)\n",
    "\n",
    "    else:\n",
    "        print(f\"Pre-computed embeddings already exist for {pub}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (diss-venv311)",
   "language": "python",
   "name": "diss-venv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
