{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ac9130-7600-414c-bc76-9b668ab8cb73",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "__INPUT__: post-semantic search subset\n",
    "\n",
    "__PROCESS__: (1) NER and (2) RE\n",
    "\n",
    "__NER__\n",
    "1. Split article_text into sentences\n",
    "2. For each sentence, quick pre-filter: take forward only sentences containing any of event_lemmas or event_phrases\n",
    "3. Extract WHAT HAPPENED: define custom pattern-matching for EVENT types via spaCy's entity ruler module in nlp pipeline, accounting for verb-form events in event_lemmas object\n",
    "4. Extract WHERE: looking for VENUE and LOCATION using GLiNER\n",
    "5. Output df with kept sentences, and NER-extracted event, venue, and location, keeping only sentences that have at least one of venue or location\n",
    "\n",
    "__RE__:\n",
    "\n",
    "6. For each row (sentence) in post-NER df, conduct dependency parsing using spaCy and convert to undirected graph using NetworkX\n",
    "7. For each NER-extracted event, locate event token/phrase (for single/multiword events respectively) in the sentence, takes that token as the 'anchor point' from which syntactic distances are calculated \n",
    "8. For each identified event in a row, find syntactically closest venue and location using find_closest_match_for_type() function, which calls nx's shortest_path_length() to get the venue/location entity\n",
    "9. Output df with added columns for matched_venue and matched_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b0bc9-b276-48d9-ae65-54eededc5311",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3dae135-6638-4354-9551-570fe8f59686",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.11/site-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc # garbage....hehe\n",
    "\n",
    "# NERcessities\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_lg\n",
    "from collections import defaultdict\n",
    "from spacy.pipeline import EntityRuler\n",
    "# !pip3 install gliner\n",
    "from gliner import GLiNER\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40598493-fb71-4c81-ad5a-0c1d4066b617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory if necessary\n",
    "# os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58ed19-8d6c-4633-982e-47cfb961ee74",
   "metadata": {},
   "source": [
    "## NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f4d037-800e-4bbd-87c2-cfa91a5605e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97be942c9cb84932a3db8ce30493f5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e270c883b4828a588941b8a4d010e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92960c1040e47b1a3183617430764b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a13fefe92c45c9a3903483550fc087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero-shot_18_table.png:   0%|          | 0.00/344k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e9c010fdb74483b62b6435f3b0ce81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "entity-types_limited.png:   0%|          | 0.00/179k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ce995e5655420aad6e2bea9b7bb715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "topics_fig_connected.png:   0%|          | 0.00/172k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb75383a88b441f08952fe3001ec79cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitignore:   0%|          | 0.00/5.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c466874a0ef64773867930a68626d3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/476 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1e83f7906c47ecabb8be68b52ab993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "countries_distribution.png:   0%|          | 0.00/398k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8135848d504fbda18f2d11d28af128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/781M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0944e4cc63bd4c868271ec098e85bd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9c613d0fc345f3bc4bff19c497740f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d41ffbee884cf59e3af0d4184a60fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nermodel = GLiNER.from_pretrained(\"EmergentMethods/gliner_medium_news-v2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fe369-6b72-4479-8424-1f42d625e8e9",
   "metadata": {},
   "source": [
    "## Big Fat Event/Location Extraction Function-defining Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94b0524-27af-41b2-ad88-009242fc8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SETTING UP\n",
    "# ==============================================================================\n",
    "\n",
    "# Defining events to look out for (both nouns and verb forms using LEMMA)\n",
    "# This is used in STEP 1 (pre-filter) and STEP 2A (spacy event extraction)\n",
    "event_lemmas = [\n",
    "    \"meeting\",        # noun\n",
    "    \"meet\",           # verb\n",
    "    \"strike\",         # noun/verb same lemma\n",
    "    \"protest\",        # noun/verb same lemma\n",
    "    \"riot\",           # noun/verb same lemma\n",
    "    \"demonstration\",  # noun\n",
    "    \"demonstrate\",    # verb\n",
    "    \"assembly\",       # noun\n",
    "    \"assemble\",       # verb\n",
    "    \"gathering\",      # noun\n",
    "    \"gather\",         # verb\n",
    "    \"lecture\"         # noun/verb same lemma\n",
    "]\n",
    "\n",
    "# Multi-word phrase (not lemmatised)\n",
    "event_phrases = [\"public meeting\"]\n",
    "\n",
    "# mapping noun and verb forms to their events so all forms of the event keyword are captured and identified as an event type\n",
    "# This is ued in STEP 2A (spacy event extraction) and in EVENT_LOC_PAIRING() to match verbs to noun event types\n",
    "nounverb_map = {\n",
    "    \"meet\": \"meeting\",\n",
    "    \"meeting\": \"meeting\",\n",
    "    \"assemble\": \"assembly\",\n",
    "    \"assembly\": \"assembly\",\n",
    "    \"gather\": \"gathering\",\n",
    "    \"gathering\": \"gathering\",\n",
    "    \"demonstrate\": \"demonstration\",\n",
    "    \"demonstration\": \"demonstration\",\n",
    "    \"riot\": \"riot\",\n",
    "    \"strike\": \"strike\",\n",
    "    \"protest\": \"protest\",\n",
    "    \"lecture\": \"lecture\"\n",
    "}\n",
    "\n",
    "gliner_labels = ['venue', 'location']\n",
    "\n",
    "# ==============================================================================\n",
    "# ENTITYRULER SETUP TO DETECT EVENTS\n",
    "# ==============================================================================\n",
    "\n",
    "# this is necessary because \"EVENT\" will not traditionally get my collective action stuff!\n",
    "\n",
    "# Create and add the EntityRuler BEFORE the statistical NER\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "event_patterns = []\n",
    "\n",
    "# Add single-token patterns for all lemmas\n",
    "for lemma in event_lemmas:\n",
    "    event_patterns.append({\"label\": \"EVENT\", \"pattern\": [{\"LEMMA\": lemma}]})\n",
    "\n",
    "# Add multi-word phrase patterns\n",
    "for phrase in event_phrases:\n",
    "    tokens = phrase.split()\n",
    "    pattern = [{\"LOWER\": tok} if i < len(tokens)-1 else {\"LEMMA\": tokens[-1]} for i, tok in enumerate(tokens)]\n",
    "    event_patterns.append({\"label\": \"EVENT\", \"pattern\": pattern})\n",
    "\n",
    "# Add patterns to the ruler\n",
    "ruler.add_patterns(event_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43c086eb-2ffd-4aa3-9698-33d1f6e36279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# NER BUILDING BLOCK FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# for step 2: FUNCTION TO CHECK IF SENTENCES CONTAIN EVENT KEYWORDS (LEMMA OR PHRASE)\n",
    "# note: this is a 'cheap' command+f type pre-filtering step\n",
    "\n",
    "def contains_event(sent, lemmas_to_check=event_lemmas, phrases_to_check=event_phrases):\n",
    "    \"\"\"\n",
    "    Checks if a spaCy sentence Span contains event lemma or phrase.\n",
    "    \"\"\"\n",
    "    text_lower = sent.text.lower()\n",
    "    # Check for multi-word phrases first\n",
    "    for phrase in phrases_to_check:\n",
    "        if phrase.lower() in text_lower:\n",
    "            return True\n",
    "            \n",
    "    # Then check for single-word lemmas\n",
    "    for token in sent:\n",
    "        if token.lemma_.lower() in lemmas_to_check:\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "    \n",
    "# for step 3: SPACY FOR EVENTS (INCL. VERB -> NOUN MAPPING) \n",
    "def extract_events(input_text):\n",
    "    \"\"\"\n",
    "    Uses SpaCy's custom rule-based EVENT extraction to identify events, both in their noun and verb forms.\n",
    "    \"\"\"\n",
    "    doc = nlp(str(input_text))\n",
    "    mapped_events = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'EVENT':\n",
    "            \n",
    "            # for \"public meeting\"\n",
    "            if ent.text.lower() in event_phrases:\n",
    "                mapped_events.append(ent.text.lower())\n",
    "                \n",
    "            # for the rest of the single-word event types\n",
    "            lemma = ent[0].lemma_.lower()\n",
    "            # Map to event label if available\n",
    "            if lemma in nounverb_map:\n",
    "                mapped_event = nounverb_map.get(lemma, ent.text)\n",
    "                mapped_events.append(mapped_event)\n",
    "\n",
    "    extracted_events = ', '.join(mapped_events) if mapped_events else None\n",
    "    return extracted_events\n",
    "\n",
    "# for step 3: GLINER FOR LOCATIONS \n",
    "def extract_locations(input_text, labels=gliner_labels, gliner_confidence=0.5):\n",
    "    \"\"\"\n",
    "    Extracts location entities (venue, location) using GLiNER.\n",
    "    Deduplicates and returns them as comma-separated strings.\n",
    "    NOTE: gliner_confidence threshold defaults to 0.5 but this can be overwritten in the master process_articles() function.\n",
    "    \"\"\"\n",
    "    entities_by_label = defaultdict(list)\n",
    "    output_dict = {}\n",
    "\n",
    "    # Extract entities using GLiNER\n",
    "    entities = nermodel.predict_entities(input_text, \n",
    "                                         labels, \n",
    "                                         threshold=gliner_confidence) \n",
    "\n",
    "    # Group entities by their label\n",
    "    for entity in entities:\n",
    "        entities_by_label[entity['label']].append(entity['text'])\n",
    "\n",
    "    # Process each label for unique, comma-separated strings\n",
    "    for label in labels:\n",
    "        if label in entities_by_label and entities_by_label[label]:\n",
    "            # Deduplicate while preserving case and order\n",
    "            unique_entities = []\n",
    "            seen_lower = set()\n",
    "            for entity in entities_by_label[label]:\n",
    "                if entity.lower() not in seen_lower:\n",
    "                    unique_entities.append(entity)\n",
    "                    seen_lower.add(entity.lower())\n",
    "            \n",
    "            output_dict[label] = \", \".join(unique_entities)\n",
    "        else:\n",
    "            output_dict[label] = None # Ensure the key exists, even if no entities were found\n",
    "            \n",
    "    return output_dict\n",
    "    \n",
    "# ==============================================================================\n",
    "# NER MASTER FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def process_articles_ner(df, gliner_confidence=0.5):\n",
    "    \"\"\"\n",
    "    1. Splits article_text for each row into sentences.\n",
    "    2. On each sentence, pre-filter contains_events() to keep sentences containing words in the event_lemmas or event_phrases list\n",
    "    3. On remaining sentence, extract EVENT, VENUE, LOCATION by calling extract_events() and extract_locations()\n",
    "    4. Keep only sentences with EVENT and at least one of VENUE/LOCATION filled\n",
    "    5. Returns: df with columns: ['corpus_id', 'sentence', 'event', 'venue', 'location']\n",
    "\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_sentences = 0\n",
    "    total_kept_sentences = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        corpus_id = row['corpus_id']\n",
    "        article = row['article_text']\n",
    "        \n",
    "        # STEP 1: SPLIT INTO SENTENCES\n",
    "        sentences = list(nlp(article).sents)\n",
    "        total_sentences += len(sentences)\n",
    "\n",
    "        \n",
    "        # STEP 2: FILTER FOR SENTENCES CONTAINING KEYWORDS \n",
    "        filtered_sents = [s for s in sentences if contains_event(s, event_lemmas, event_phrases)]\n",
    "        total_kept_sentences += len(filtered_sents)\n",
    "\n",
    "        for sent in filtered_sents:\n",
    "            sent_text = sent.text.strip()\n",
    "            \n",
    "            # STEP 3: Extract events (from spaCy EntityRuler)\n",
    "            event_str = extract_events(sent_text)\n",
    "\n",
    "            \n",
    "            # STEP 4: Extract locations (from GLiNER)\n",
    "            loc_dict = extract_locations(sent_text, \n",
    "                             labels=['venue', 'location'], # Pass both labels\n",
    "                             gliner_confidence=gliner_confidence)\n",
    "            \n",
    "            # Append result\n",
    "            results.append({\n",
    "               \"corpus_id\": corpus_id,\n",
    "                \"sentence\": sent_text,\n",
    "                \"event\": event_str,\n",
    "                \"venue\": loc_dict.get(\"venue\"), # Get venue from the dictionary\n",
    "                \"location\": loc_dict.get(\"location\") # Get location from the dictionary\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # STEP 5: Keep only rows where both event and at least venue or location are filled \n",
    "    result_df_dropped = result_df[result_df['event'].notna() & \n",
    "                        (result_df['venue'].notna() | result_df['location'].notna())].copy()\n",
    "\n",
    "    # === Print how many sentences were dropped for not containing event_lemma or event_phrase ===\n",
    "    total_dropped = total_sentences - total_kept_sentences\n",
    "    print(f\"\\nTotal sentences dropped (no event keywords): {total_dropped} out of {total_sentences} ({total_dropped/total_sentences:.2%})\")\n",
    "    \n",
    "    # === Print how many rows were dropped because of missing venue/location ===\n",
    "    n_rows_dropped = len(result_df)-len(result_df_dropped)\n",
    "    print(f\"Rows dropped due to missing event/location: {n_rows_dropped} out of {len(result_df)} ({n_rows_dropped/len(result_df):.2%})\")\n",
    "    \n",
    "    return result_df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "290fd4b5-d011-4bf0-b4ef-511ad9880606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RE BUILDING BLOCK FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# for step 6: function to build dependency graph from sentence\n",
    "def build_dependency_graph(doc):\n",
    "    \"\"\"Convert a spaCy Doc into an undirected graph where nodes are token indices.\"\"\"\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((token.i, child.i))\n",
    "    return nx.Graph(edges)\n",
    "\n",
    "# for step 7: function to look for multiword events (i.e. public meeting in this case)\n",
    "def match_span_indices(doc, phrase):\n",
    "    \"\"\"Finds token indices for a multiword entity (case-insensitive). e.g. Chartist Lecture Room\"\"\"\n",
    "    phrase_tokens = phrase.lower().split()\n",
    "    for i in range(len(doc) - len(phrase_tokens) + 1):\n",
    "        window = [t.text.lower() for t in doc[i:i+len(phrase_tokens)]]\n",
    "        if window == phrase_tokens:\n",
    "            return list(range(i, i+len(phrase_tokens)))\n",
    "\n",
    "\n",
    "# for step 8: function to find best-candidate venue/location with the shortest syntactic path to the event token\n",
    "def find_closest_match_for_type(event_head_token, entity_list, doc, graph):\n",
    "    \"\"\"\n",
    "    Finds the entity from a given list that is syntactically closest to a given event token.\n",
    "\n",
    "    Args:\n",
    "        event_head_token (spacy.Token): The head token of the event entity.\n",
    "        entity_list (list): A list of entity strings to search through (e.g., all venues).\n",
    "        doc (spacy.Doc): The spaCy Doc object for the sentence.\n",
    "        graph (nx.Graph): The dependency graph for the sentence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_entity_text, min_distance)\n",
    "               - best_entity_text (str|None): The text of the closest entity found.\n",
    "               - min_distance (float): The shortest dependency path length.\n",
    "    \"\"\"\n",
    "    best_entity = None\n",
    "    min_distance = float(\"inf\")\n",
    "\n",
    "    if not entity_list:\n",
    "        return None, float(\"inf\")\n",
    "\n",
    "    for entity_text in entity_list:\n",
    "        span_indices = match_span_indices(doc, entity_text)\n",
    "        if not span_indices:\n",
    "            continue\n",
    "\n",
    "        # Get the syntactic head of the entity span to represent it in the graph\n",
    "        entity_span = doc[span_indices[0]:span_indices[-1] + 1]\n",
    "        entity_head_idx = entity_span.root.i\n",
    "\n",
    "        try:\n",
    "            # Calculate the shortest path from the event's head to the entity's head\n",
    "            path_length = nx.shortest_path_length(graph, source=event_head_token.i, target=entity_head_idx)\n",
    "\n",
    "            if path_length < min_distance:\n",
    "                min_distance = path_length\n",
    "                best_entity = entity_text\n",
    "        except nx.NetworkXNoPath:\n",
    "            # No syntactic path exists between the event and this entity\n",
    "            continue\n",
    "            \n",
    "    return best_entity, min_distance\n",
    "\n",
    "# ==============================================================================\n",
    "# RE MASTER FUNCTION\n",
    "# ==============================================================================\n",
    "def event_loc_pairing(df, max_path_len=4):\n",
    "    \"\"\"\n",
    "    For each event in a sentence, finds the closest 'venue' AND the closest 'location'\n",
    "    independently, based on the shortest syntactic path.\n",
    "\n",
    "    It creates one row per event, containing any matched venue and/or location that\n",
    "    falls within the max_path_len threshold.\n",
    "    \n",
    "    Returns a new DataFrame with the paired results.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # running over each row in df and...\n",
    "    for _, row in df.iterrows():\n",
    "        sentence_text = row[\"sentence\"]\n",
    "\n",
    "        # spacy parsing for dependency parsing\n",
    "        doc = nlp(sentence_text)\n",
    "\n",
    "        # defining the graph for the current sentence(doc)\n",
    "        graph = build_dependency_graph(doc)\n",
    "\n",
    "        # Create lists from comma-separated strings, handling potential None values\n",
    "        events = [e.strip() for e in str(row[\"event\"]).split(',') if e.strip()] if pd.notna(row[\"event\"]) else []\n",
    "        venues = [v.strip() for v in str(row[\"venue\"]).split(',') if v.strip()] if pd.notna(row[\"venue\"]) else []\n",
    "        locations = [l.strip() for l in str(row[\"location\"]).split(',') if l.strip()] if pd.notna(row[\"location\"]) else []\n",
    "\n",
    "        # If there are no locations/venues to match, skip this sentence. \n",
    "        # more of a second safety check since we've already dropped rows with no venue nor location in the NER step\n",
    "        if not venues and not locations:\n",
    "            continue\n",
    "            \n",
    "        # find syntactically closest venue and location for each event in events\n",
    "        for event in events:\n",
    "            # initialising\n",
    "            event_head_token = None\n",
    "            \n",
    "            # MULTI-WORD phrase matching for \"public meeting\" \n",
    "            span_indices = match_span_indices(doc, event)\n",
    "            if span_indices:\n",
    "                event_head_token = doc[span_indices[0]:span_indices[-1] + 1].root # if found, this takes the syntactic 'anchor' of the phrase\n",
    "            \n",
    "            # SINGLE-WORD event types: for each sentence token, check if its lemma matches the event string\n",
    "            else:\n",
    "                for token in doc:\n",
    "                    mapped_token_event = nounverb_map.get(token.lemma_.lower())\n",
    "                    if mapped_token_event == event.lower():\n",
    "                        event_head_token = token\n",
    "                        break # Stop after finding the first match\n",
    "            \n",
    "            # If we still couldn't find any representation of the event, skip it.\n",
    "            if not event_head_token:\n",
    "                continue\n",
    "\n",
    "            ### NOTE: event_head_token is the \"anchor point\" from which syntactic distance is calculated in the following step.  \n",
    "\n",
    "            # --- Find the closest VENUE independently ---\n",
    "            best_venue, venue_dist = find_closest_match_for_type(event_head_token, venues, doc, graph)\n",
    "            \n",
    "            # --- Find the closest LOCATION independently ---\n",
    "            best_location, loc_dist = find_closest_match_for_type(event_head_token, locations, doc, graph)\n",
    "\n",
    "            # Apply the max_path_len filter to each match\n",
    "            matched_venue = best_venue if venue_dist <= max_path_len else None\n",
    "            matched_location = best_location if loc_dist <= max_path_len else None\n",
    "\n",
    "            # Only create a row if at least one successful pairing was made\n",
    "            if matched_venue or matched_location:\n",
    "                row_data = row.to_dict()\n",
    "                row_data[\"event\"] = event  # The specific event for this row\n",
    "                row_data[\"re_venue\"] = matched_venue\n",
    "                row_data[\"re_location\"] = matched_location\n",
    "            \n",
    "                results.append(row_data)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d694470-3e20-4a46-a773-d9cafffe4308",
   "metadata": {},
   "source": [
    "## Trying it out on first 5 rows of subset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d68e94-b790-4774-9f85-851435ed05e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>article_text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178361</td>\n",
       "      <td>0.839721</td>\n",
       "      <td>1838-09-08</td>\n",
       "      <td>star</td>\n",
       "      <td>Preparatory to a Demonstration in favor of the...</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>454650</td>\n",
       "      <td>0.839554</td>\n",
       "      <td>1842-02-26</td>\n",
       "      <td>star</td>\n",
       "      <td>The London Chartists are auxiously invited to ...</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>798998</td>\n",
       "      <td>0.832106</td>\n",
       "      <td>1839-05-04</td>\n",
       "      <td>star</td>\n",
       "      <td>PUBLIC MEETING. - In this small village we had...</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>709266</td>\n",
       "      <td>0.830941</td>\n",
       "      <td>1842-04-30</td>\n",
       "      <td>star</td>\n",
       "      <td>Islington.—A public open air meeting was held ...</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>833213</td>\n",
       "      <td>0.828974</td>\n",
       "      <td>1841-10-02</td>\n",
       "      <td>star</td>\n",
       "      <td>LEEDS.—On Sunday last, in the absence of Mr. M...</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus_id     score        date source  \\\n",
       "0     178361  0.839721  1838-09-08   star   \n",
       "1     454650  0.839554  1842-02-26   star   \n",
       "2     798998  0.832106  1839-05-04   star   \n",
       "3     709266  0.830941  1842-04-30   star   \n",
       "4     833213  0.828974  1841-10-02   star   \n",
       "\n",
       "                                        article_text  year  \n",
       "0  Preparatory to a Demonstration in favor of the...  1838  \n",
       "1  The London Chartists are auxiously invited to ...  1842  \n",
       "2  PUBLIC MEETING. - In this small village we had...  1839  \n",
       "3  Islington.—A public open air meeting was held ...  1842  \n",
       "4  LEEDS.—On Sunday last, in the absence of Mr. M...  1841  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# please = subset.head(n=5)\n",
    "# please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5150a3db-0a8e-49a0-ba5b-761fd11b40e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences dropped (no event keywords): 28 out of 56 (50.00%)\n",
      "Rows dropped due to missing event/location: 17 out of 28 (60.71%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>event</th>\n",
       "      <th>venue</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178361</td>\n",
       "      <td>On Monday evening last, the 3rd instant, the W...</td>\n",
       "      <td>assembly, demonstration</td>\n",
       "      <td>None</td>\n",
       "      <td>Hull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178361</td>\n",
       "      <td>The chairman then read a letter received on Su...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Old Palace Yard</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>454650</td>\n",
       "      <td>This will be the most important meeting ever h...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>454650</td>\n",
       "      <td>F. O'Connor will address a general meeting of ...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Social Institution</td>\n",
       "      <td>John-street, Tottenham-Court-road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>454650</td>\n",
       "      <td>MEN of BIRMINGHAM.—A meeting will be held in t...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Town Hall</td>\n",
       "      <td>BIRMINGHAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>798998</td>\n",
       "      <td>At the time appointed for the meeting taking p...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>New Mills, Glos- sop, Hyde, Stockport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>798998</td>\n",
       "      <td>When they arrived at Marple Bridge they all pr...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>Marple Bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>798998</td>\n",
       "      <td>The above meeting was addressed by Messrs. T. ...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>Stalybridge, Hyde, Glossop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>709266</td>\n",
       "      <td>Islington.—A public open air meeting was held ...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>Islington, Finsbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>833213</td>\n",
       "      <td>HALIFAX.—O'Connor Demonstration.—The committee...</td>\n",
       "      <td>demonstration</td>\n",
       "      <td>None</td>\n",
       "      <td>HALIFAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>833213</td>\n",
       "      <td>The committee meet every Tuesday evening, at t...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Chartist Lecture Room</td>\n",
       "      <td>Swan Coppic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpus_id                                           sentence  \\\n",
       "1      178361  On Monday evening last, the 3rd instant, the W...   \n",
       "4      178361  The chairman then read a letter received on Su...   \n",
       "6      454650  This will be the most important meeting ever h...   \n",
       "7      454650  F. O'Connor will address a general meeting of ...   \n",
       "8      454650  MEN of BIRMINGHAM.—A meeting will be held in t...   \n",
       "12     798998  At the time appointed for the meeting taking p...   \n",
       "13     798998  When they arrived at Marple Bridge they all pr...   \n",
       "16     798998  The above meeting was addressed by Messrs. T. ...   \n",
       "18     709266  Islington.—A public open air meeting was held ...   \n",
       "24     833213  HALIFAX.—O'Connor Demonstration.—The committee...   \n",
       "25     833213  The committee meet every Tuesday evening, at t...   \n",
       "\n",
       "                      event                  venue  \\\n",
       "1   assembly, demonstration                   None   \n",
       "4                   meeting        Old Palace Yard   \n",
       "6                   meeting                   None   \n",
       "7                   meeting     Social Institution   \n",
       "8                   meeting              Town Hall   \n",
       "12                  meeting                   None   \n",
       "13                  meeting                   None   \n",
       "16                  meeting                   None   \n",
       "18                  meeting                   None   \n",
       "24            demonstration                   None   \n",
       "25                  meeting  Chartist Lecture Room   \n",
       "\n",
       "                                 location  \n",
       "1                                    Hull  \n",
       "4                             Westminster  \n",
       "6                                  London  \n",
       "7       John-street, Tottenham-Court-road  \n",
       "8                              BIRMINGHAM  \n",
       "12  New Mills, Glos- sop, Hyde, Stockport  \n",
       "13                          Marple Bridge  \n",
       "16             Stalybridge, Hyde, Glossop  \n",
       "18                    Islington, Finsbury  \n",
       "24                                HALIFAX  \n",
       "25                            Swan Coppic  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pls = process_articles_ner(please, gliner_confidence=0.7)\n",
    "# pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ff2665d-f6b0-4d7d-ad8b-cca7a8770660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>event</th>\n",
       "      <th>venue</th>\n",
       "      <th>location</th>\n",
       "      <th>re_venue</th>\n",
       "      <th>re_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178361</td>\n",
       "      <td>On Monday evening last, the 3rd instant, the W...</td>\n",
       "      <td>assembly</td>\n",
       "      <td>None</td>\n",
       "      <td>Hull</td>\n",
       "      <td>None</td>\n",
       "      <td>Hull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178361</td>\n",
       "      <td>The chairman then read a letter received on Su...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Old Palace Yard</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>Old Palace Yard</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>454650</td>\n",
       "      <td>This will be the most important meeting ever h...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>454650</td>\n",
       "      <td>F. O'Connor will address a general meeting of ...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Social Institution</td>\n",
       "      <td>John-street, Tottenham-Court-road</td>\n",
       "      <td>Social Institution</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709266</td>\n",
       "      <td>Islington.—A public open air meeting was held ...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>None</td>\n",
       "      <td>Islington, Finsbury</td>\n",
       "      <td>None</td>\n",
       "      <td>Islington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>833213</td>\n",
       "      <td>The committee meet every Tuesday evening, at t...</td>\n",
       "      <td>meeting</td>\n",
       "      <td>Chartist Lecture Room</td>\n",
       "      <td>Swan Coppic</td>\n",
       "      <td>Chartist Lecture Room</td>\n",
       "      <td>Swan Coppic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus_id                                           sentence     event  \\\n",
       "0     178361  On Monday evening last, the 3rd instant, the W...  assembly   \n",
       "1     178361  The chairman then read a letter received on Su...   meeting   \n",
       "2     454650  This will be the most important meeting ever h...   meeting   \n",
       "3     454650  F. O'Connor will address a general meeting of ...   meeting   \n",
       "4     709266  Islington.—A public open air meeting was held ...   meeting   \n",
       "5     833213  The committee meet every Tuesday evening, at t...   meeting   \n",
       "\n",
       "                   venue                           location  \\\n",
       "0                   None                               Hull   \n",
       "1        Old Palace Yard                        Westminster   \n",
       "2                   None                             London   \n",
       "3     Social Institution  John-street, Tottenham-Court-road   \n",
       "4                   None                Islington, Finsbury   \n",
       "5  Chartist Lecture Room                        Swan Coppic   \n",
       "\n",
       "                re_venue  re_location  \n",
       "0                   None         Hull  \n",
       "1        Old Palace Yard  Westminster  \n",
       "2                   None       London  \n",
       "3     Social Institution         None  \n",
       "4                   None    Islington  \n",
       "5  Chartist Lecture Room  Swan Coppic  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pls_postre = event_loc_pairing(pls, max_path_len=4)\n",
    "# pls_postre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c48a9b-5163-49dd-a3df-5ee911651215",
   "metadata": {},
   "source": [
    "## Saving test output to test mapping stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a564ed4-a3cf-4437-bb78-74e47eb2cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pls_postre.to_csv(\"data/postre_test.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d9e3b-9d4e-43d2-b331-916287f9d3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check = subset.loc[subset['corpus_id'] == 798998]\n",
    "# check['article_text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3a7cb-cc4f-4391-9362-d24f41db1b81",
   "metadata": {},
   "source": [
    "## Moment of truth af.... let's try on my whole semantic-searched subset (this was run as a script on CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6066a0b0-7a95-4c03-a647-5708966cec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>article_text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178361</td>\n",
       "      <td>0.839721</td>\n",
       "      <td>1838-09-08</td>\n",
       "      <td>star</td>\n",
       "      <td>Preparatory to a Demonstration in favor of the...</td>\n",
       "      <td>1838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>454650</td>\n",
       "      <td>0.839554</td>\n",
       "      <td>1842-02-26</td>\n",
       "      <td>star</td>\n",
       "      <td>The London Chartists are auxiously invited to ...</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>798998</td>\n",
       "      <td>0.832106</td>\n",
       "      <td>1839-05-04</td>\n",
       "      <td>star</td>\n",
       "      <td>PUBLIC MEETING. - In this small village we had...</td>\n",
       "      <td>1839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>709266</td>\n",
       "      <td>0.830941</td>\n",
       "      <td>1842-04-30</td>\n",
       "      <td>star</td>\n",
       "      <td>Islington.—A public open air meeting was held ...</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>833213</td>\n",
       "      <td>0.828974</td>\n",
       "      <td>1841-10-02</td>\n",
       "      <td>star</td>\n",
       "      <td>LEEDS.—On Sunday last, in the absence of Mr. M...</td>\n",
       "      <td>1841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus_id     score        date source  \\\n",
       "0     178361  0.839721  1838-09-08   star   \n",
       "1     454650  0.839554  1842-02-26   star   \n",
       "2     798998  0.832106  1839-05-04   star   \n",
       "3     709266  0.830941  1842-04-30   star   \n",
       "4     833213  0.828974  1841-10-02   star   \n",
       "\n",
       "                                        article_text  year  \n",
       "0  Preparatory to a Demonstration in favor of the...  1838  \n",
       "1  The London Chartists are auxiously invited to ...  1842  \n",
       "2  PUBLIC MEETING. - In this small village we had...  1839  \n",
       "3  Islington.—A public open air meeting was held ...  1842  \n",
       "4  LEEDS.—On Sunday last, in the absence of Mr. M...  1841  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = pd.read_csv(\"data/ner_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c39a75-6b44-4ce8-bb1b-0f48774c7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running both NER and RE functions\n",
    "ner_re_output = event_loc_pairing(process_articles_ner(subset, gliner_confidence=0.7))\n",
    "\n",
    "# saving locally\n",
    "ner_re_output.to_csv(\"data/ner_re_done_v2.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f28aa-6321-44c0-9749-1dcec417b300",
   "metadata": {},
   "source": [
    "## Script output:\n",
    "Total sentences dropped (no event keywords): 459183 out of 602545 (76.21%)\n",
    "\n",
    "Rows dropped due to missing event/location: 74172 out of 143362 (51.74%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
